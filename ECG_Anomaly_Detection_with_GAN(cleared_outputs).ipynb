{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYrumdl6TbxL"
      },
      "source": [
        "### 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJixLEz8YcZd"
      },
      "outputs": [],
      "source": [
        "!pip install wfdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCdKYqzSTJmW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import wfdb  # PhysioNet's Waveform Database library\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeQW55Y6XxBX"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR_uyb1aXxRN"
      },
      "outputs": [],
      "source": [
        "!wget -r -N -c -np https://physionet.org/files/mitdb/1.0.0/\n",
        "!mv physionet.org/files/mitdb/1.0.0 ./mitdb  # Move files to a simpler path\n",
        "!rm -r physionet.org  # Clean up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmLhcWucTf_I"
      },
      "source": [
        "### 2. Load MIT-BIH Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M91KF8nKTgLU"
      },
      "outputs": [],
      "source": [
        "# List all records (e.g., '100', '101', ...)\n",
        "records = [f.split('.')[0] for f in os.listdir('mitdb') if f.endswith('.dat')]\n",
        "\n",
        "# Load signals and annotations\n",
        "def load_ecg_record(record_name):\n",
        "    signal = wfdb.rdrecord(f'mitdb/{record_name}').p_signal[:, 0]  # Lead II (MLII)\n",
        "    annotation = wfdb.rdann(f'mitdb/{record_name}', 'atr')\n",
        "    return signal, annotation\n",
        "\n",
        "# Example: Load record\n",
        "ecg, ann = load_ecg_record('100')\n",
        "print(len(ecg))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List all records (e.g., '100', '101', ...)\n",
        "records = [f.split('.')[0] for f in os.listdir('mitdb') if f.endswith('.dat')]\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wfdb\n",
        "\n",
        "def load_ecg_with_labels(record_name, window_size=256):\n",
        "    # Load ECG and annotations\n",
        "    signal = wfdb.rdrecord(f'mitdb/{record_name}').p_signal[:, 0]  # Lead II\n",
        "    annotation = wfdb.rdann(f'mitdb/{record_name}', 'atr')\n",
        "\n",
        "    segments = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop over signal in fixed-size windows\n",
        "    for start in range(0, len(signal) - window_size, window_size):\n",
        "        end = start + window_size\n",
        "        segment = signal[start:end]\n",
        "\n",
        "        # Find beats within this window\n",
        "        beat_indices = np.where((annotation.sample >= start) & (annotation.sample < end))[0]\n",
        "        beat_labels = [annotation.symbol[i] for i in beat_indices]\n",
        "\n",
        "        # Label as anomaly if any beat != 'N'\n",
        "        label = 0 if all(b == 'N' for b in beat_labels) else 1\n",
        "\n",
        "        segments.append(segment)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        \"ECG_Segment\": segments,\n",
        "        \"Label\": labels\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "df = load_ecg_with_labels('100', window_size=256)\n",
        "print(df.head())\n",
        "print(df['Label'].value_counts())\n"
      ],
      "metadata": {
        "id": "yAcqGRei8xPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic EDA"
      ],
      "metadata": {
        "id": "wPrKTVtYNuvR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLwDeEkZgCmT"
      },
      "outputs": [],
      "source": [
        "symbols, counts = np.unique(ann.symbol, return_counts=True)\n",
        "readable_counts = {str(sym): int(cnt) for sym, cnt in zip(symbols, counts)}\n",
        "print(\"Symbols and their counts: \",readable_counts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Show dataset distribution\n",
        "# ------------------------------\n",
        "total_samples = len(df['Label'])\n",
        "normal_samples = np.sum(df['Label'] == 0)\n",
        "anomaly_samples = np.sum(df['Label'] == 1)\n",
        "\n",
        "print(f\"Total ECG Segments: {total_samples}\")\n",
        "print(f\"Normal Segments: {normal_samples}\")\n",
        "print(f\"Anomaly Segments: {anomaly_samples}\")\n"
      ],
      "metadata": {
        "id": "w6l7ZLfBUwyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,4))\n",
        "plt.bar([\"Normal\", \"Anomaly\"], [normal_samples, anomaly_samples], color=[\"green\", \"red\"])\n",
        "plt.title(\"ECG Data Distribution\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eOrOOxwxUxGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Pick first segment and flatten\n",
        "segment = df[\"ECG_Segment\"].iloc[0]  # This is already a 1D array\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(segment)\n",
        "plt.title(\"First ECG Segment\")\n",
        "plt.xlabel(\"Sample\")\n",
        "plt.ylabel(\"Amplitude (mV)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ena9n88RN6kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqXyaa8DLTM4"
      },
      "outputs": [],
      "source": [
        "# List all .dat files (each corresponds to one record)\n",
        "records = [f.split('.')[0] for f in os.listdir('mitdb') if f.endswith('.dat')]\n",
        "print(f\"Found {len(records)} records: {records[:5]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrwSfDZVTgXK"
      },
      "source": [
        "### 3. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OHVEch4UVWI"
      },
      "source": [
        "3.1 Segment ECG into Normal Beats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXqFytElTgi-"
      },
      "outputs": [],
      "source": [
        "all_segments = []\n",
        "window_size = 256\n",
        "\n",
        "for record in records:\n",
        "    try:\n",
        "        # Load ECG and annotations\n",
        "        ecg, ann = load_ecg_record(record)  # Your existing function\n",
        "\n",
        "        # Extract segments (normal beats 'N' + other valid labels)\n",
        "        normal_labels = ['N', 'L', 'R', 'e', 'j']  # Normal variants\n",
        "        r_peaks = ann.sample[np.isin(ann.symbol, normal_labels)]\n",
        "\n",
        "        for peak in r_peaks:\n",
        "            start = max(0, peak - window_size//2)\n",
        "            end = start + window_size\n",
        "            if end <= len(ecg):\n",
        "                segment = ecg[start:end]\n",
        "                all_segments.append(segment)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing {record}: {e}\")\n",
        "        continue\n",
        "\n",
        "all_segments = np.array(all_segments)\n",
        "print(f\"✅ Total segments extracted: {len(all_segments)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame for beats\n",
        "df_beats = pd.DataFrame(all_segments)\n",
        "\n",
        "print(\"\\nShape of segmented beats array:\", df_beats.shape)\n",
        "print(\"First beat segment:\\n\", df_beats.iloc[0])"
      ],
      "metadata": {
        "id": "xnRGQn9AOnGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(df_beats[:1000])\n",
        "plt.title(\"ECG Signal Snippet\")\n",
        "plt.xlabel(\"Sample\")\n",
        "plt.ylabel(\"Amplitude (mV)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o_qU_QgzPXen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBT8FqGTTgt1"
      },
      "source": [
        "3.2 Normalize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvwrB_qhTg6-"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Normalize all segments together\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "segments_normalized = scaler.fit_transform(all_segments.reshape(-1, window_size)).reshape(-1, window_size, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segments_normalized[:10]"
      ],
      "metadata": {
        "id": "J3QpdnWRO3kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset before and after preprocessing"
      ],
      "metadata": {
        "id": "XFH7puGOSTDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wfdb\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# -----------------------------\n",
        "# Load raw ECG signal (Record 100)\n",
        "# -----------------------------\n",
        "record_name = \"100\"\n",
        "signal = wfdb.rdrecord(f\"mitdb/{record_name}\").p_signal[:, 0]  # Lead II\n",
        "annotation = wfdb.rdann(f\"mitdb/{record_name}\", \"atr\")\n",
        "\n",
        "# Create DataFrame for raw dataset (first 20 samples for display)\n",
        "raw_df = pd.DataFrame({\n",
        "    \"Index\": range(20),\n",
        "    \"ECG_Value\": signal[:20],\n",
        "    \"Annotation\": [annotation.symbol[i] if i < len(annotation.symbol) else None for i in range(20)]\n",
        "})\n",
        "\n",
        "print(\"Raw Dataset (Record 100 - first 10 samples):\")\n",
        "display(raw_df.head(10))\n",
        "print(\"\\n\\n\")\n",
        "# -----------------------------\n",
        "# Preprocessing: Segment & Label\n",
        "# -----------------------------\n",
        "window_size = 256\n",
        "df_preprocessed = load_ecg_with_labels(record_name=\"100\", window_size=window_size)\n",
        "\n",
        "# Keep only NORMAL segments\n",
        "df_normal = df_preprocessed[df_preprocessed[\"Label\"] == 0].reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Normalization (Min-Max scaling to [-1, 1])\n",
        "# -----------------------------\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "segments_array = np.stack(df_normal[\"ECG_Segment\"].values)  # shape: (n_samples, window_size)\n",
        "segments_normalized = scaler.fit_transform(segments_array)\n",
        "\n",
        "# Add normalized data back into DataFrame\n",
        "df_normal[\"Normalized_Segment\"] = list(segments_normalized)\n",
        "# --- create a clean table (first 10 values of each normalized segment) ---\n",
        "normalized_df = pd.DataFrame(\n",
        "    segments_normalized[:, :10],  # take first 10 values for readability\n",
        "    columns=[f\"Val_{i+1}\" for i in range(10)]\n",
        ")\n",
        "\n",
        "# (Optional) Add the Label column for clarity\n",
        "normalized_df[\"Label\"] = df_normal[\"Label\"].values\n",
        "print(\"Preprocessed ECG Segments (showing first 10 values of each segment):\")\n",
        "display(normalized_df.head(10))\n"
      ],
      "metadata": {
        "id": "yisMJR2sSTUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display Anomaly data"
      ],
      "metadata": {
        "id": "L2aOcpcwlMiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'df' contains record 100 ECG data with 'ECG_Segment' and 'Label' columns\n",
        "# Label = 0 -> Normal, 1 -> Anomalous\n",
        "\n",
        "# --- Filter anomalous segments ---\n",
        "df_anomaly = df[df[\"Label\"] == 1].reset_index(drop=True)\n",
        "\n",
        "# --- Convert segments into array ---\n",
        "segments_array = np.stack(df_anomaly[\"ECG_Segment\"].values)\n",
        "\n",
        "# --- Create DataFrame (show first 10 values for readability) ---\n",
        "anomaly_df = pd.DataFrame(\n",
        "    segments_array[:, :10],  # first 10 values of each anomalous segment\n",
        "    columns=[f\"Val_{i+1}\" for i in range(10)]\n",
        ")\n",
        "\n",
        "# Add annotation/label info\n",
        "anomaly_df[\"Label\"] = df_anomaly[\"Label\"].values\n",
        "\n",
        "print(\"Anomalous ECG Segments from Record 100 (first 10 values shown):\")\n",
        "display(anomaly_df.head(10))\n"
      ],
      "metadata": {
        "id": "uGEJrNuTlM0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2bSXe61ThDv"
      },
      "source": [
        "3.3 Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K9xkdraThSk"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = train_test_split(segments_normalized, test_size=0.1, random_state=42)\n",
        "X_train = X_train.reshape(-1, window_size, 1)  # Add channel dim for Conv1D\n",
        "X_test = X_test.reshape(-1, window_size, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E-jaaDpThcH"
      },
      "source": [
        "### 4. Build GAN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sbQReC9ThqU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Reshape, Conv1DTranspose, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_generator(latent_dim=100, window_size=256):\n",
        "    noise = Input(shape=(latent_dim,))\n",
        "\n",
        "    x = Dense(64)(noise)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Dense(128)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Dense(window_size)(x)\n",
        "    x = Reshape((window_size, 1))(x)\n",
        "\n",
        "    return Model(noise, x)\n",
        "\n",
        "generator = build_generator()\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I78EDs36lqmd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import (Input, Conv1D, LeakyReLU, LayerNormalization,\n",
        "                                    Dropout, GlobalMaxPooling1D, Dense)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_stable_discriminator(window_size=256):\n",
        "    ecg_input = Input(shape=(window_size, 1))\n",
        "\n",
        "    # Feature extraction with spectral normalization\n",
        "    x = Conv1D(64, 5, strides=2, padding='same')(ecg_input)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    x = LayerNormalization()(x)  # Better than BatchNorm for GANs\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Conv1D(128, 5, strides=2, padding='same')(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Conv1D(256, 5, strides=2, padding='same')(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    # Decision layer\n",
        "    x = GlobalMaxPooling1D()(x)  # More stable than Flatten()\n",
        "    x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    discriminator = Model(ecg_input, x)\n",
        "\n",
        "    # Use lower learning rate for discriminator\n",
        "    optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "    discriminator.compile(loss='binary_crossentropy',\n",
        "                         optimizer=optimizer,\n",
        "                         metrics=['accuracy'])\n",
        "    return discriminator\n",
        "\n",
        "# Usage\n",
        "discriminator = build_stable_discriminator(window_size=256)\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsbqCfeklva_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Freeze discriminator during generator training\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Connect generator -> discriminator\n",
        "gan_input = Input(shape=(100,))\n",
        "fake_ecg = generator(gan_input)\n",
        "gan_output = discriminator(fake_ecg)\n",
        "\n",
        "# Compile GAN\n",
        "gan = Model(gan_input, gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYhQShqrU537"
      },
      "source": [
        "### 5. Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X64M6Hs0U6DQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def train_gan(X_train, generator, discriminator, gan,\n",
        "             epochs, batch_size, window_size=256):\n",
        "    # Adversarial ground truths\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        # Select random real ECGs\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        real_ecgs = X_train[idx]\n",
        "\n",
        "        # Generate fake ECGs\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "        fake_ecgs = generator.predict(noise)\n",
        "\n",
        "        # Train discriminator\n",
        "        d_loss_real = discriminator.train_on_batch(real_ecgs, valid)\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_ecgs, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "        g_loss = gan.train_on_batch(noise, valid)\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 1000 == 0:\n",
        "            print(f\"Epoch {epoch} | D Loss: {d_loss[0]:.4f} | D Acc: {100*d_loss[1]:.2f}% | G Loss: {g_loss:.4f}\")\n",
        "\n",
        "\n",
        "# Run training with your existing data\n",
        "train_gan(\n",
        "    X_train=X_train,  # Your preprocessed training data (shape: [n_samples, window_size, 1])\n",
        "    generator=generator,\n",
        "    discriminator=discriminator,\n",
        "    gan=gan,\n",
        "    epochs=1000,\n",
        "    batch_size=64,     # Adjust based on GPU memory\n",
        "    window_size=256    # Must match your segment length\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXHBUAUzU6-n"
      },
      "source": [
        "### 6. Reconstruction and Anomaly Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EwCKzMhVc56"
      },
      "source": [
        "6.1 Compute Reconstruction Error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOfw0dcuU7MD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Force TensorFlow to use GPU\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "        print(\"✅ Using GPU for TensorFlow operations\")\n",
        "    except:\n",
        "        pass\n",
        "else:\n",
        "    print(\"⚠️ No GPU found — running on CPU\")\n",
        "\n",
        "def ecg_to_latent(real_ecg, generator, iterations=500):\n",
        "    \"\"\"Find latent code that best reconstructs the input ECG using GPU for predictions\"\"\"\n",
        "    latent_dim = generator.input_shape[1]\n",
        "    real_ecg = real_ecg.reshape(1, -1, 1).astype(np.float32)\n",
        "\n",
        "    # Loss function to minimize\n",
        "    def loss(z):\n",
        "        z_tensor = tf.convert_to_tensor(z.reshape(1, latent_dim), dtype=tf.float32)\n",
        "        generated = generator(z_tensor, training=False)  # GPU inference\n",
        "        return float(tf.reduce_mean(tf.square(generated - real_ecg)).numpy())\n",
        "\n",
        "    # Optimize latent vector (CPU)\n",
        "    result = minimize(loss,\n",
        "                      x0=np.random.randn(latent_dim),\n",
        "                      method='L-BFGS-B',\n",
        "                      options={'maxiter': iterations})\n",
        "    return result.x\n",
        "\n",
        "def reconstruct_with_gan(ecg_signal, generator):\n",
        "    \"\"\"Generate reconstructed ECG using GPU inference\"\"\"\n",
        "    latent_code = ecg_to_latent(ecg_signal, generator)\n",
        "    z_tensor = tf.convert_to_tensor(latent_code.reshape(1, -1), dtype=tf.float32)\n",
        "    reconstructed = generator(z_tensor, training=False)\n",
        "    return reconstructed.numpy()[0]\n",
        "\n",
        "# Example usage\n",
        "original_ecg = X_test[0]  # Your normalized ECG segment\n",
        "reconstructed = reconstruct_with_gan(original_ecg, generator)\n",
        "mse = np.mean((original_ecg - reconstructed) ** 2)\n",
        "print(f\"Reconstruction MSE: {mse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJleXFQ2Vf0q"
      },
      "source": [
        "6.2 Visualize Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wkdzt72uVgDe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_reconstruction(original, reconstructed, title=\"\"):\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(original.flatten(), label='Original', linewidth=2)\n",
        "    plt.plot(reconstructed.flatten(), label='Reconstructed', linestyle='--')\n",
        "    plt.title(f\"{title} (MSE: {np.mean((original-reconstructed)**2):.4f}\")\n",
        "    plt.legend()\n",
        "    plt.xlabel('Samples')\n",
        "    plt.ylabel('Amplitude (normalized)')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize normal vs abnormal reconstructions\n",
        "normal_ecg = X_test[0]  # Replace with your normal ECG\n",
        "abnormal_ecg = X_test[np.argmax(discriminator.predict(X_test))]  # Most anomalous\n",
        "\n",
        "for ecg, label in [(normal_ecg, \"Normal ECG\"),\n",
        "                   (abnormal_ecg, \"Abnormal ECG\")]:\n",
        "    reconstructed = reconstruct_with_gan(ecg, generator)\n",
        "    plot_reconstruction(ecg, reconstructed, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C96I_itmVfcM"
      },
      "source": [
        "6.3 Detect Anomalies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKuZY5lQVfp8"
      },
      "outputs": [],
      "source": [
        "def gan_anomaly_detector(ecg_signal, generator, discriminator, threshold=0.3):\n",
        "    \"\"\"Combined anomaly score using reconstruction error and discriminator\"\"\"\n",
        "    # Reconstruction error\n",
        "    reconstructed = reconstruct_with_gan(ecg_signal, generator)\n",
        "    mse = np.mean((ecg_signal - reconstructed)**2)\n",
        "\n",
        "    # Discriminator confidence\n",
        "    realness = discriminator.predict(ecg_signal.reshape(1, -1, 1))[0][0]\n",
        "\n",
        "    # Combined score (weighted)\n",
        "    anomaly_score = 0.7*mse + 0.3*(1-realness)  # Adjust weights as needed\n",
        "\n",
        "    if anomaly_score > threshold:\n",
        "        print(f\"🚨 Anomaly Detected (Score: {anomaly_score:.3f})\")\n",
        "        print(f\" - Reconstruction MSE: {mse:.4f}\")\n",
        "        print(f\" - Discriminator 'real' confidence: {realness*100:.1f}%\")\n",
        "    else:\n",
        "        print(f\"✅ Normal ECG (Score: {anomaly_score:.3f})\")\n",
        "\n",
        "    return anomaly_score\n",
        "\n",
        "# Example usage\n",
        "abnormal_ecg, _ = load_ecg_record('124')\n",
        "abnormal_segment = abnormal_ecg[1000:1000+256]  # Window of 256 samples\n",
        "abnormal_segment = scaler.transform(abnormal_segment.reshape(1, -1)).reshape(1, 256, 1)\n",
        "gan_anomaly_detector(abnormal_segment, generator, discriminator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdGpoLKRVx2O"
      },
      "source": [
        "### 7. Save & Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gOUuhjpVyHl"
      },
      "outputs": [],
      "source": [
        "generator.save('generator.h5')  # For later use"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix"
      ],
      "metadata": {
        "id": "BukQp04YzHV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import wfdb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Load ECG Record + Annotation\n",
        "# ------------------------------\n",
        "def load_ecg_record(record_name):\n",
        "    signal = wfdb.rdrecord(f\"mitdb/{record_name}\").p_signal[:, 0]  # Lead II (MLII)\n",
        "    annotation = wfdb.rdann(f\"mitdb/{record_name}\", 'atr')\n",
        "    return signal, annotation\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Segment ECG & Create Labels\n",
        "# ------------------------------\n",
        "def get_labels_from_annotations(signal, ann, window_size=256):\n",
        "    normal_beats = ['N']  # only 'N' considered normal\n",
        "    labels = []\n",
        "    segments = []\n",
        "\n",
        "    for start in range(0, len(signal) - window_size, window_size):\n",
        "        seg = signal[start:start+window_size]\n",
        "        beat_indices = [i for i, s in enumerate(ann.sample) if start <= s < start+window_size]\n",
        "        beat_symbols = [ann.symbol[i] for i in beat_indices]\n",
        "\n",
        "        if any(sym not in normal_beats for sym in beat_symbols):\n",
        "            label = 1   # anomaly\n",
        "        else:\n",
        "            label = 0   # normal\n",
        "\n",
        "        segments.append(seg)\n",
        "        labels.append(label)\n",
        "\n",
        "    return np.array(segments), np.array(labels)\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Load & Process Data\n",
        "# ------------------------------\n",
        "ecg, ann = load_ecg_record('100')  # Change record ID as needed\n",
        "segments, labels = get_labels_from_annotations(ecg, ann, window_size=256)\n",
        "\n",
        "print(f\"Segments shape: {segments.shape}\")\n",
        "print(f\"Labels distribution  [normal, anomaly]: {np.bincount(labels)}\")\n",
        "print(f\"Total Segments: {len(segments)}\")\n",
        "\n",
        "# ------------------------------\n",
        "# Train only on Normal ECGs\n",
        "# ------------------------------\n",
        "normal_indices = np.where(labels == 0)[0]\n",
        "anomaly_indices = np.where(labels == 1)[0]\n",
        "\n",
        "segments_normal = segments[normal_indices]\n",
        "labels_normal = labels[normal_indices]\n",
        "\n",
        "# Normalize (fit only on normal data)\n",
        "scaler = StandardScaler()\n",
        "segments_normalized = scaler.fit_transform(segments_normal)\n",
        "\n",
        "# Train/test split for NORMAL data only\n",
        "X_train, X_test_normal, y_train, y_test_normal = train_test_split(\n",
        "    segments_normalized, labels_normal, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Keep anomalies only for testing\n",
        "X_test_anomaly = segments[anomaly_indices]\n",
        "y_test_anomaly = labels[anomaly_indices]\n",
        "\n",
        "# Normalize anomalies using the same scaler\n",
        "X_test_anomaly = scaler.transform(X_test_anomaly)\n",
        "\n",
        "# Combine normal test + anomaly test\n",
        "X_test = np.vstack([X_test_normal, X_test_anomaly])\n",
        "y_test = np.hstack([y_test_normal, y_test_anomaly])\n",
        "\n",
        "# Reshape for Conv1D\n",
        "window_size = 256\n",
        "X_train = X_train.reshape(-1, window_size, 1)\n",
        "X_test = X_test.reshape(-1, window_size, 1)\n",
        "\n",
        "# Info\n",
        "print(f\"\\nTraining set contains ONLY normal ECGs:\")\n",
        "print(f\"   X_train: {len(X_train)}, y_train distribution: {np.bincount(y_train)}\")\n",
        "\n",
        "print(f\"\\nTesting set contains normal + anomalies:\")\n",
        "print(f\"   X_test: {len(X_test)}, y_test distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 4. Load Generator\n",
        "# ------------------------------\n",
        "generator = load_model(\"generator.h5\", compile=False)\n",
        "\n",
        "# ------------------------------\n",
        "# 5. GPU-Optimized Reconstruction Function\n",
        "# ------------------------------\n",
        "def reconstruct_with_gan_gpu(ecg_signal, generator, steps=50, lr=0.05):\n",
        "    latent_dim = generator.input_shape[1]\n",
        "    target = tf.convert_to_tensor(ecg_signal.reshape(1, -1, 1), dtype=tf.float32)\n",
        "\n",
        "    z = tf.Variable(tf.random.normal([1, latent_dim], dtype=tf.float32))\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    for _ in range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "            generated = generator(z, training=False)\n",
        "            loss = tf.reduce_mean(tf.square(generated - target))\n",
        "        grads = tape.gradient(loss, [z])\n",
        "        optimizer.apply_gradients(zip(grads, [z]))\n",
        "\n",
        "    return generator(z, training=False)[0].numpy()\n",
        "\n",
        "# ------------------------------\n",
        "# 6. Compute Reconstruction Errors\n",
        "# ------------------------------\n",
        "errors = []\n",
        "for i in range(len(X_test)):\n",
        "    reconstructed = reconstruct_with_gan_gpu(X_test[i], generator, steps=50)\n",
        "    mse = np.mean((X_test[i] - reconstructed)**2)\n",
        "    errors.append(mse)\n",
        "errors = np.array(errors)\n",
        "\n",
        "# ------------------------------\n",
        "# 7. Evaluation (Confusion Matrix & Metrics)\n",
        "# ------------------------------\n",
        "threshold = np.percentile(errors, 70)  # select threshold\n",
        "y_pred = (errors > threshold).astype(int)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Normal\", \"Anomaly\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "mkZrAs_nzHg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### confusion matrix for high threshold(90%)"
      ],
      "metadata": {
        "id": "1jBTf8dEgWoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------\n",
        "# 90th Percentile Evaluation\n",
        "# ------------------------------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "print(f\"Segments shape: {segments.shape}\")\n",
        "print(f\"Labels distribution  [normal, anomaly]: {np.bincount(labels)}\")\n",
        "print(f\"Total Segments: {len(segments)}\")\n",
        "\n",
        "print(f\"\\nTraining set contains ONLY normal ECGs:\")\n",
        "print(f\"   X_train: {len(X_train)}, y_train distribution: {np.bincount(y_train)}\")\n",
        "\n",
        "print(f\"\\nTesting set contains normal + anomalies:\")\n",
        "print(f\"   X_test: {len(X_test)}, y_test distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "\n",
        "# Calculate 90th percentile threshold\n",
        "threshold = np.percentile(errors, 90)  # 90th percentile\n",
        "y_pred = (errors > threshold).astype(int)\n",
        "\n",
        "# Confusion Matrix\n",
        "fig, ax = plt.subplots(figsize=(5,5))  # smaller figure prevents hanging\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Normal\", \"Anomaly\"])\n",
        "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "JDM_KC2IOUt8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}